{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List\n",
    "\n",
    "os.environ['HF_HOME'] = '/scratch/' + str(open('../tokens/HPC_ACCOUNT_ID.txt', 'r').read())\n",
    "cache_dir = '/scratch/' + str(open('../tokens/HPC_ACCOUNT_ID.txt', 'r').read()) + '/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c0ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"LeoZotos/bio_full\"\n",
    "WIKI = \"simple\" # 'en' or 'simple'. 'en' crashes on interactive node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_api_key = \"\"\n",
    "with open(\"../tokens/HF_TOKEN.txt\", \"r\") as f:\n",
    "    hf_api_key = f.read().strip()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a706e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_set = load_dataset(DATASET, split='train', token = hf_api_key, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9452e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_docs(\n",
    "    question_set: Dataset,\n",
    "    passages: Dataset,\n",
    "    device: torch.device,\n",
    "    top_k: int = 20,\n",
    "    batch_size: int = 204800, # Adjusted for more realistic starting point\n",
    "    num_workers: int = 48,) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    !!!This function was written by Gemini 2.5!!!\n",
    "    Retrieves the top_k relevant documents for each question using an optimized,\n",
    "    asynchronous DataLoader to fix CPU bottlenecks.\n",
    "\n",
    "    Args:\n",
    "        question_set: A Hugging Face Dataset containing questions. Each row must have an 'emb' column\n",
    "                      with the question embedding.\n",
    "        passages: A Hugging Face Dataset of passages. Each row must have an 'emb' column\n",
    "                  with the passage embedding, and 'title' and 'text' columns.\n",
    "        device: The torch.device to run the computation on (e.g., torch.device('cuda:0')).\n",
    "        top_k: The number of top documents to retrieve for each question.\n",
    "        batch_size: The number of passages to process in a single batch. Adjust based on GPU memory.\n",
    "                    A larger batch_size is faster but uses more memory.\n",
    "        num_workers: The number of CPU processes to use for data loading.\n",
    "                     A value > 0 enables asynchronous data loading.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each inner list contains the top_k relevant documents\n",
    "        (formatted as \"title text\") for a corresponding question.\n",
    "    \"\"\"\n",
    "    # Important: Set the format for the passages dataset *once* before creating the DataLoader.\n",
    "    # We only need the 'emb' column for the computation loop.\n",
    "    passages.set_format(\n",
    "        \"torch\",\n",
    "        columns=['emb'],\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    # 1. Load all question embeddings into GPU memory.\n",
    "    num_questions = len(question_set)\n",
    "    question_embs = torch.tensor(question_set['emb'], dtype=torch.bfloat16).to(device)\n",
    "\n",
    "    # 2. Initialize tensors to store top scores and their corresponding passage indices.\n",
    "    top_k_scores = torch.full((num_questions, top_k), -torch.inf, device=device, dtype=torch.bfloat16)\n",
    "    top_k_indices = torch.full((num_questions, top_k), -1, device=device, dtype=torch.long)\n",
    "\n",
    "    # 3. Set up the DataLoader for efficient, asynchronous data loading.\n",
    "    #    - pin_memory=True speeds up CPU-to-GPU transfers.\n",
    "    #    - num_workers > 0 uses background processes to load data, so the GPU doesn't wait.\n",
    "    passage_loader = DataLoader(\n",
    "        passages,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False  # Ensure all passages are processed\n",
    "    )\n",
    "\n",
    "    # 4. Iterate through the passages using the new efficient DataLoader.\n",
    "    global_passage_idx = 0\n",
    "    for batch in tqdm(passage_loader, desc=\"Finding top passages\"):\n",
    "        # `batch` is now a dictionary {'emb': tensor_on_cpu}.\n",
    "        # The DataLoader has already prepared this batch in the background.\n",
    "        # .to(device, non_blocking=True) starts the GPU transfer and returns immediately.\n",
    "        passage_embs_batch = batch['emb'].to(device, non_blocking=True)\n",
    "\n",
    "        # The GPU can start computing on the previous batch while this one is transferring.\n",
    "        # (Though in this specific loop, the benefit is mainly that the CPU can\n",
    "        # start preparing the *next* batch while this one transfers).\n",
    "        batch_scores = torch.matmul(question_embs, passage_embs_batch.T)\n",
    "\n",
    "        combined_scores = torch.cat([top_k_scores, batch_scores], dim=1)\n",
    "        top_k_scores, relative_indices = torch.topk(combined_scores, top_k, dim=1)\n",
    "\n",
    "        # Use the actual batch size, which might be smaller for the last batch\n",
    "        current_batch_size = passage_embs_batch.shape[0]\n",
    "        batch_global_indices = torch.arange(\n",
    "            global_passage_idx, global_passage_idx + current_batch_size, device=device\n",
    "        ).expand(num_questions, -1)\n",
    "\n",
    "        combined_indices = torch.cat([top_k_indices, batch_global_indices], dim=1)\n",
    "        top_k_indices = torch.gather(combined_indices, 1, relative_indices)\n",
    "\n",
    "        global_passage_idx += current_batch_size\n",
    "    \n",
    "    # Before the final step, reset the format to get 'title' and 'text'\n",
    "    passages.reset_format()\n",
    "\n",
    "    # 5. Retrieve the documents using the final top_k_indices.\n",
    "    top_k_indices_cpu = top_k_indices.cpu().numpy()\n",
    "\n",
    "    relevant_docs_combined = []\n",
    "    print(\"Retrieving final documents...\")\n",
    "    for q_indices in tqdm(top_k_indices_cpu, desc=\"Formatting results\"):\n",
    "        valid_indices = [idx for idx in q_indices if idx != -1]\n",
    "        # Fetching documents one by one can be slow, but it's correct.\n",
    "        # For very large datasets, batch fetching could be another optimization.\n",
    "        docs = passages[valid_indices]\n",
    "        formatted_docs = [f\"{title} {text}\" for title, text in zip(docs['title'], docs['text'])]\n",
    "        relevant_docs_combined.append(formatted_docs)\n",
    "\n",
    "    return relevant_docs_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = load_dataset(\"Cohere/wikipedia-2023-11-embed-multilingual-v3\", WIKI, split=\"train\", cache_dir=cache_dir, token=hf_api_key)\n",
    "\n",
    "relevant_docs = retrieve_relevant_docs(question_set, passages, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_column_data(example, idx):\n",
    "    example[\"Relevant_Docs_Simple\"] = relevant_docs[idx]\n",
    "    return example\n",
    "\n",
    "question_set = question_set.map(replace_column_data, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f5e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question_set['Question_With_Options'][3])\n",
    "print(\"-----\")\n",
    "print(question_set['Relevant_Docs_Simple'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_set.push_to_hub(\n",
    "    repo_id=DATASET,\n",
    "    commit_message=\"Added relevant documents from Wiki Simple\",\n",
    "    token=hf_api_key,\n",
    "    private=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distractor_preference_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
