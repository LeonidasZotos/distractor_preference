{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statannotations.Annotator import Annotator\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_HOME'] = '/scratch/' + str(open('../tokens/HPC_ACCOUNT_ID.txt', 'r').read())\n",
    "cache_dir = '/scratch/' + str(open('../tokens/HPC_ACCOUNT_ID.txt', 'r').read()) + '/cache'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sentence_bert_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"LeoZotos/immu_full\"\n",
    "WIKI = \"En\" # 'En' or 'Simple'\n",
    "SOURCE_TEXT = \"\"  # '_Only_Options' # or '' for full text\n",
    "NUM_DOCS_RETRIEVED = 60 # 20 or 60\n",
    "\n",
    "HAS_CONTENT_DISTRACTORS = 2 # 0 to 2, -1 for any\n",
    "SHORT_OPTIONS_THRESHOLD = 10000 # >500 for all options, otherwise 20 or so for short options\n",
    "\n",
    "RETRIEVED_DOCS_COL_NAME = 'Relevant_Docs_' + WIKI + SOURCE_TEXT + '_' + str(NUM_DOCS_RETRIEVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f245ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nan_or_none(x):\n",
    "    return x is None or (isinstance(x, float) and np.isnan(x))\n",
    "\n",
    "def has_short_options(list_of_options, threshold=20):\n",
    "    non_empty_options = [option for option in list_of_options if option and not is_nan_or_none(option)]\n",
    "    avg_length = sum(len(option) for option in non_empty_options) / len(non_empty_options)\n",
    "    return avg_length < threshold\n",
    "\n",
    "data = load_dataset(DATASET, split='train', cache_dir=cache_dir)\n",
    "print(\"Before filtering, dataset size:\", len(data))\n",
    "data = data.filter(lambda x: \n",
    "    not is_nan_or_none(x['Answer_A_Rate']) and\n",
    "    not is_nan_or_none(x['Answer_B_Rate']) and\n",
    "    not is_nan_or_none(x['Answer_C_Rate']) and\n",
    "    not is_nan_or_none(x['Answer_D_Rate'])  # add a not to exclude 4 choice questions\n",
    "    )\n",
    "\n",
    "if HAS_CONTENT_DISTRACTORS in [0, 1, 2]:\n",
    "    data = data.filter(lambda x: x['Has_Content_Distractors'] == HAS_CONTENT_DISTRACTORS)\n",
    "\n",
    "# Filter out entries based on length of options\n",
    "if SHORT_OPTIONS_THRESHOLD < 500:\n",
    "    data = data.filter(lambda x: not has_short_options([x['Answer_A'], x['Answer_B'], x['Answer_C'], x['Answer_D']], SHORT_OPTIONS_THRESHOLD))\n",
    "\n",
    "print(\"After filtering, dataset size:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c624008",
   "metadata": {},
   "source": [
    "# Classify retrieved docs per choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6aaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similarities(similarities, sentences1, sentences2):\n",
    "    for idx_i, sentence1 in enumerate(sentences1):\n",
    "        print(sentence1)\n",
    "        for idx_j, sentence2 in enumerate(sentences2):\n",
    "            print(f\" - {sentence2: <30}: {similarities[idx_i][idx_j]:.4f}\")\n",
    "\n",
    "\n",
    "def classify_docs_per_distractor(row, sentence_bert_model):\n",
    "    docs_per_choice = {}\n",
    "    for key in ['Answer_A', 'Answer_B', 'Answer_C', 'Answer_D']:\n",
    "        if row[key] != \"\":\n",
    "                docs_per_choice[key + '_Docs'] = []\n",
    "    choices_keys = [key[:-5] for key in docs_per_choice.keys()]\n",
    "    choices_content = [row[key] for key in choices_keys if row[key] != \"\"]\n",
    "    embeddings_choices = sentence_bert_model.encode(choices_content)\n",
    "    embeddings_docs = sentence_bert_model.encode(row[RETRIEVED_DOCS_COL_NAME])\n",
    "    similarities = sentence_bert_model.similarity(embeddings_choices, embeddings_docs)\n",
    "\n",
    "    # We now add each doc to the choice with the highest similarity\n",
    "    for i, doc in enumerate(row[RETRIEVED_DOCS_COL_NAME]):\n",
    "        max_sim_index = np.argmax(similarities[:, i])\n",
    "        max_choice = list(docs_per_choice.keys())[max_sim_index]\n",
    "        docs_per_choice[max_choice].append(doc)\n",
    "    \n",
    "    return docs_per_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [f\"Answer_{choice}_Docs\" for choice in ['A', 'B', 'C', 'D']]\n",
    "docs_by_choice = {name: [] for name in column_names}\n",
    "\n",
    "for row in tqdm(data):\n",
    "    docs_per_choice_for_row = classify_docs_per_distractor(row, sentence_bert_model)\n",
    "    for name in column_names:\n",
    "        docs_by_choice[name].append(docs_per_choice_for_row.get(name, []))\n",
    "        \n",
    "if column_names[0] in data.column_names:\n",
    "    data = data.remove_columns(column_names)\n",
    "    \n",
    "for name, column_data in docs_by_choice.items():\n",
    "    data = data.add_column(name, column_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect an instance manually to see if it makes sense\n",
    "id = 1\n",
    "print(data[id]['Question_With_Options'], \":\", \"\\n A:\", data[id]['Answer_A_Docs'], \"\\n B:\", data[id]['Answer_B_Docs'], \"\\n C:\", data[id]['Answer_C_Docs'], \"\\n D:\", data[id]['Answer_D_Docs'])\n",
    "\n",
    "# print the lengths of the lists, only for id\n",
    "print(f\"Lengths for ID {id}:\")\n",
    "for name in column_names:\n",
    "    print(f\"{name}: {len(data[id][name])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678be9a4",
   "metadata": {},
   "source": [
    "# Evaluate Difference between number of docs retrieved per choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff88689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_significance_lengths(data):\n",
    "    lengths_per_choice = {\n",
    "        'Answer_A': [len(x['Answer_A_Docs']) for x in data],\n",
    "        'Answer_B': [len(x['Answer_B_Docs']) for x in data],\n",
    "        'Answer_C': [len(x['Answer_C_Docs']) for x in data],\n",
    "        'Answer_D': [len(x['Answer_D_Docs']) for x in data],\n",
    "    }\n",
    "    if sum(lengths_per_choice['Answer_D']) < 20:  # Check if there are actual data points for Answer_D\n",
    "        # If not, remove it from the dictionary\n",
    "        lengths_per_choice.pop('Answer_D')\n",
    "        print(lengths_per_choice.keys(), \"does not contain Answer_D, removing it from the analysis.\")\n",
    "        \n",
    "    for key, value in lengths_per_choice.items():\n",
    "        print(f\"{key} - Mean: {np.mean(value)}, Std: {np.std(value)}, Min: {np.min(value)}, Max: {np.max(value)}\")\n",
    "    \n",
    "    return lengths_per_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392da1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_per_choice = test_significance_lengths(data)\n",
    "\n",
    "data_tuples = []\n",
    "for choice, lengths in lengths_per_choice.items():\n",
    "    for length in lengths:\n",
    "        data_tuples.append((choice, length))\n",
    "\n",
    "df = pd.DataFrame(data_tuples, columns=['Choice', 'Length'])\n",
    "\n",
    "choices = lengths_per_choice.keys()\n",
    "comparison_pairs = list(combinations(choices, 2))\n",
    "\n",
    "print(\"Pairs to be compared:\", comparison_pairs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "sns.boxplot(data=df, x='Choice', y='Length', ax=ax, hue='Choice')\n",
    "\n",
    "annotator = Annotator(ax, comparison_pairs, data=df, x='Choice', y='Length')\n",
    "annotator.configure(test='t-test_ind', text_format='star', verbose=2)\n",
    "\n",
    "annotator.apply_test().annotate()\n",
    "\n",
    "ax.set_title('Pairwise Comparisons of Number of Passages Retrieved per Answer Choice', fontsize=16)\n",
    "ax.set_xlabel('Answer Choice', fontsize=12)\n",
    "ax.set_ylabel('Retrieved Passages per choice', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d25c41",
   "metadata": {},
   "source": [
    "# Calculate Correlation between choice's selection rate and docs retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bcb6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_rates_with_correct = []\n",
    "doc_lengths_with_correct = []\n",
    "print(lengths_per_choice.keys())\n",
    "for row in data:\n",
    "    for choice in lengths_per_choice.keys():\n",
    "        selection_rates_with_correct.append(row[choice + '_Rate'])\n",
    "        doc_lengths_with_correct.append(len(row[choice + '_Docs']))\n",
    "\n",
    "correlations_with_correct = {\n",
    "    'Pearson': pearsonr(selection_rates_with_correct, doc_lengths_with_correct),\n",
    "    'Spearman': spearmanr(selection_rates_with_correct, doc_lengths_with_correct)\n",
    "}\n",
    "print(\"Correlations with correct answers:\")\n",
    "print(f\"Pearson: {correlations_with_correct['Pearson'][0]:.4f} (p={correlations_with_correct['Pearson'][1]:.4f})\")\n",
    "print(f\"Spearman: {correlations_with_correct['Spearman'][0]:.4f} (p={correlations_with_correct['Spearman'][1]:.4f})\")\n",
    "\n",
    "\n",
    "selection_rates_without_correct = []\n",
    "doc_lengths_without_correct = []\n",
    "choices = [choice for choice in lengths_per_choice.keys() if choice != \"Answer_A\"]\n",
    "for row in data:\n",
    "    for choice in choices:\n",
    "        selection_rates_without_correct.append(row[choice + '_Rate'])\n",
    "        doc_lengths_without_correct.append(len(row[choice + '_Docs']))\n",
    "\n",
    "correlations_without_correct = {\n",
    "    'Pearson': pearsonr(selection_rates_without_correct, doc_lengths_without_correct),\n",
    "    'Spearman': spearmanr(selection_rates_without_correct, doc_lengths_without_correct)\n",
    "}\n",
    "print(\"Correlations without correct answers:\")\n",
    "print(f\"Pearson: {correlations_without_correct['Pearson'][0]:.4f} (p={correlations_without_correct['Pearson'][1]:.4f})\")\n",
    "print(f\"Spearman: {correlations_without_correct['Spearman'][0]:.4f} (p={correlations_without_correct['Spearman'][1]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f1d4f",
   "metadata": {},
   "source": [
    "# Calculate Correlation within each choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b01f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_correlation(type='pearson'):\n",
    "    simple_list = []\n",
    "    print(type.capitalize(), \"correlation between distractor rates and document lengths(A-D, p-values in between):\")\n",
    "    correlations_with_docs_len = {}\n",
    "\n",
    "    for choice_name in [f\"Answer_{choice}\" for choice in ['A', 'B', 'C', 'D']]:\n",
    "        rates = data[f'{choice_name}_Rate']\n",
    "        doc_lengths = [len(sentence_list) for sentence_list in data[f'{choice_name}_Docs']]\n",
    "        correlation, p = None, None\n",
    "        # count nones in rates and doc_lengths\n",
    "        print(f\"Choice: {choice_name}, Nones in rates: {sum(is_nan_or_none(rate) for rate in rates)}, Nones in doc_lengths: {sum(is_nan_or_none(length) for length in doc_lengths)}\")\n",
    "        \n",
    "        # if rates has nones ,continue\n",
    "        if any(is_nan_or_none(rate) for rate in rates):\n",
    "            correlation, p = 0, 0\n",
    "        else:\n",
    "            if type == 'pearson':\n",
    "                correlation, p = pearsonr(rates, doc_lengths)\n",
    "            elif type == 'spearman':\n",
    "                correlation, p = spearmanr(rates, doc_lengths)\n",
    "        \n",
    "        correlations_with_docs_len[choice_name] = (round(correlation,4), round(p,4))\n",
    "        simple_list.append(float(round(correlation,4)))\n",
    "        simple_list.append(float(round(p,4)))\n",
    "        \n",
    "    correlations_string = \"\\t\".join(\n",
    "        [f\"{str(correlation)} {str(p)}\"\n",
    "         for (correlation, p) in correlations_with_docs_len.values()]\n",
    "    )  \n",
    "    print(correlations_string)\n",
    "        \n",
    "    return simple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "all_results.extend(calc_correlation('pearson'))\n",
    "all_results.extend(calc_correlation('spearman'))\n",
    "\n",
    "# Export to csv\n",
    "df = pd.DataFrame([all_results], columns=[\n",
    "    'Pearson_A_Correlation', 'Pearson_A_p', \n",
    "    'Pearson_B_Correlation', 'Pearson_B_p', \n",
    "    'Pearson_C_Correlation', 'Pearson_C_p', \n",
    "    'Pearson_D_Correlation', 'Pearson_D_p',\n",
    "    'Spearman_A_Correlation', 'Spearman_A_p', \n",
    "    'Spearman_B_Correlation', 'Spearman_B_p', \n",
    "    'Spearman_C_Correlation', 'Spearman_C_p', \n",
    "    'Spearman_D_Correlation', 'Spearman_D_p'\n",
    "])\n",
    "df.to_csv('last_correlations.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distractor_preference_env (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
