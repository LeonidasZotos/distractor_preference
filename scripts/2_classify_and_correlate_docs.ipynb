{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['HF_HOME'] = '/scratch/' + str(open('../tokens/HPC_ACCOUNT_ID.txt', 'r').read())\n",
    "cache_dir = '/scratch/' + str(open('../tokens/HPC_ACCOUNT_ID.txt', 'r').read()) + '/cache'\n",
    "hf_api_key = \"\"\n",
    "with open(\"../tokens/HF_TOKEN.txt\", \"r\") as f:\n",
    "    hf_api_key = f.read().strip()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sentence_bert_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"LeoZotos/bio_full\"\n",
    "WIKI = \"Simple\" # 'En' or 'Simple'\n",
    "SOURCE_TEXT = \"_Only_Options\"  # '_Only_Options' # or '' for full text\n",
    "NUM_DOCS_RETRIEVED = 60 # 20 or 60\n",
    "\n",
    "HAS_CONTENT_DISTRACTORS = 2 # 0 to 2, -1 for any\n",
    "SHORT_OPTIONS_THRESHOLD = 10000 # >500 for all options, otherwise 20 or so for short options\n",
    "\n",
    "RETRIEVED_DOCS_COL_NAME = 'Relevant_Docs_' + WIKI + SOURCE_TEXT + '_' + str(NUM_DOCS_RETRIEVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f245ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nan_or_none(x):\n",
    "    return x is None or (isinstance(x, float) and np.isnan(x))\n",
    "\n",
    "def has_short_options(list_of_options, threshold=20):\n",
    "    non_empty_options = [option for option in list_of_options if option and not is_nan_or_none(option)]\n",
    "    avg_length = sum(len(option) for option in non_empty_options) / len(non_empty_options)\n",
    "    return avg_length < threshold\n",
    "\n",
    "data = load_dataset(DATASET, split='train', token=hf_api_key, cache_dir=cache_dir)\n",
    "print(\"Before filtering, dataset size:\", len(data))\n",
    "data = data.filter(lambda x: \n",
    "    not is_nan_or_none(x['Answer_A_Rate']) and\n",
    "    not is_nan_or_none(x['Answer_B_Rate']) and\n",
    "    not is_nan_or_none(x['Answer_C_Rate']) and\n",
    "    not is_nan_or_none(x['Answer_D_Rate']) \n",
    "    )\n",
    "\n",
    "if HAS_CONTENT_DISTRACTORS in [0, 1, 2]:\n",
    "    data = data.filter(lambda x: x['Has_Content_Distractors'] == HAS_CONTENT_DISTRACTORS)\n",
    "\n",
    "# Filter out entries based on length of options\n",
    "if SHORT_OPTIONS_THRESHOLD < 500:\n",
    "    data = data.filter(lambda x: not has_short_options([x['Answer_A'], x['Answer_B'], x['Answer_C'], x['Answer_D']], SHORT_OPTIONS_THRESHOLD))\n",
    "\n",
    "print(\"After filtering, dataset size:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c624008",
   "metadata": {},
   "source": [
    "# Classify retrieved docs per choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6aaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similarities(similarities, sentences1, sentences2):\n",
    "    for idx_i, sentence1 in enumerate(sentences1):\n",
    "        print(sentence1)\n",
    "        for idx_j, sentence2 in enumerate(sentences2):\n",
    "            print(f\" - {sentence2: <30}: {similarities[idx_i][idx_j]:.4f}\")\n",
    "\n",
    "\n",
    "def classify_docs_per_distractor(row, sentence_bert_model):\n",
    "    docs_per_choice = {}\n",
    "    for key in ['Answer_A', 'Answer_B', 'Answer_C', 'Answer_D']:\n",
    "        if row[key] != \"\":\n",
    "                docs_per_choice[key + '_Docs'] = []\n",
    "    choices_keys = [key[:-5] for key in docs_per_choice.keys()]\n",
    "    choices_content = [row[key] for key in choices_keys if row[key] != \"\"]\n",
    "    embeddings_choices = sentence_bert_model.encode(choices_content)\n",
    "    embeddings_docs = sentence_bert_model.encode(row[RETRIEVED_DOCS_COL_NAME])\n",
    "    similarities = sentence_bert_model.similarity(embeddings_choices, embeddings_docs)\n",
    "\n",
    "    # We now add each doc to the choice with the highest similarity\n",
    "    for i, doc in enumerate(row[RETRIEVED_DOCS_COL_NAME]):\n",
    "        max_sim_index = np.argmax(similarities[:, i])\n",
    "        max_choice = list(docs_per_choice.keys())[max_sim_index]\n",
    "        docs_per_choice[max_choice].append(doc)\n",
    "    \n",
    "    return docs_per_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [f\"Answer_{choice}_Docs\" for choice in ['A', 'B', 'C', 'D']]\n",
    "docs_by_choice = {name: [] for name in column_names}\n",
    "\n",
    "for row in tqdm(data):\n",
    "    docs_per_choice_for_row = classify_docs_per_distractor(row, sentence_bert_model)\n",
    "    for name in column_names:\n",
    "        docs_by_choice[name].append(docs_per_choice_for_row.get(name, []))\n",
    "        \n",
    "if column_names[0] in data.column_names:\n",
    "    data = data.remove_columns(column_names)\n",
    "    \n",
    "for name, column_data in docs_by_choice.items():\n",
    "    data = data.add_column(name, column_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect an instance manually to see if it makes sense\n",
    "id = 22\n",
    "print(data[id]['Question_With_Options'], \":\", \"\\n A:\", data[id]['Answer_A_Docs'], \"\\n B:\", data[id]['Answer_B_Docs'], \"\\n C:\", data[id]['Answer_C_Docs'], \"\\n D:\", data[id]['Answer_D_Docs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f1d4f",
   "metadata": {},
   "source": [
    "# Calculate Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b01f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_correlation(type='pearson'):\n",
    "    simple_list = []\n",
    "    print(type.capitalize(), \"correlation between distractor rates and document lengths(A-D, p-values in between):\")\n",
    "    correlations_with_docs_len = {}\n",
    "\n",
    "    for choice_name in [f\"Answer_{choice}\" for choice in ['A', 'B', 'C', 'D']]:\n",
    "        rates = data[f'{choice_name}_Rate']\n",
    "        doc_lengths = [len(sentence_list) for sentence_list in data[f'{choice_name}_Docs']]\n",
    "        correlation, p = None, None\n",
    "        # count nones in rates and doc_lengths\n",
    "        print(f\"Choice: {choice_name}, Nones in rates: {sum(is_nan_or_none(rate) for rate in rates)}, Nones in doc_lengths: {sum(is_nan_or_none(length) for length in doc_lengths)}\")\n",
    "        \n",
    "        # if rates has nones ,continue\n",
    "        if any(is_nan_or_none(rate) for rate in rates):\n",
    "            correlation, p = 0, 0\n",
    "        else:\n",
    "            if type == 'pearson':\n",
    "                correlation, p = pearsonr(rates, doc_lengths)\n",
    "            elif type == 'spearman':\n",
    "                correlation, p = spearmanr(rates, doc_lengths)\n",
    "        \n",
    "        correlations_with_docs_len[choice_name] = (round(correlation,4), round(p,4))\n",
    "        simple_list.append(float(round(correlation,4)))\n",
    "        simple_list.append(float(round(p,4)))\n",
    "        \n",
    "    correlations_string = \"\\t\".join(\n",
    "        [f\"{str(correlation)} {str(p)}\"\n",
    "         for (correlation, p) in correlations_with_docs_len.values()]\n",
    "    )  \n",
    "    print(correlations_string)\n",
    "        \n",
    "    return simple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "all_results.extend(calc_correlation('pearson'))\n",
    "all_results.extend(calc_correlation('spearman'))\n",
    "\n",
    "# export to csv\n",
    "df = pd.DataFrame([all_results], columns=[\n",
    "    'Pearson_A_Correlation', 'Pearson_A_p', \n",
    "    'Pearson_B_Correlation', 'Pearson_B_p', \n",
    "    'Pearson_C_Correlation', 'Pearson_C_p', \n",
    "    'Pearson_D_Correlation', 'Pearson_D_p',\n",
    "    'Spearman_A_Correlation', 'Spearman_A_p', \n",
    "    'Spearman_B_Correlation', 'Spearman_B_p', \n",
    "    'Spearman_C_Correlation', 'Spearman_C_p', \n",
    "    'Spearman_D_Correlation', 'Spearman_D_p'\n",
    "])\n",
    "df.to_csv('last_correlations.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distractor_preference_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
